# -*- coding: utf-8 -*-
"""trainer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jzC-jR71ywUScSkGP8BEMz2iHh_ibIaK
"""

#  !pip install tiktoken
import tiktoken
import torch
import time
from Transformer import DecoderBlock
import torch.nn as nn
import torch.nn.functional as F

tokenizer = tiktoken.get_encoding('cl100k_base')

# reading-data
with open('data/input.txt',  'r') as f:
    text = f.read()
#-------

# train/val split
enc_text = tokenizer.encode(text)
split = int(0.9 * len(enc_text))
train_data = enc_text[:split]
val_data = enc_text[split:]
#------

# hyperparameters

vocab_size = tokenizer.n_vocab
device = 'cuda' if torch.cuda.is_available() else 'cpu'
n_embd = 32
block_size = 128
batch_size = 32
learning_rate = 1e-4
steps = 20000
eval_step = steps // 10


# data-loading-function
def data_loader(split='train'):
    data = train_data if split=='train' else val_data
    rand_nums = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([torch.tensor(data[i:i+block_size]) for i in rand_nums]).to(device)
    y = torch.stack([torch.tensor(data[i+1:i+block_size+1]) for i in rand_nums]).to(device)
    return x, y


# estimated-loss
@torch.no_grad
def get_loss(m):
    train_lossi = []
    val_lossi = []
    m.eval()
    for _ in range(100):
        # train
        x, y = data_loader('train')
        logits, loss = m(x, y)
        train_lossi.append(loss)

        # val
        x, y = data_loader('val')
        logits, loss = m(x, y)
        val_lossi.append(loss)
    train_loss = torch.tensor(train_lossi).mean()
    val_loss = torch.tensor(val_lossi).mean()
    m.train()
    return train_loss, val_loss


# gpt-model
class GPT(nn.Module):

    '''The language model that that is trained and used to generate text'''

    def __init__(self, vocab_size, n_embd=16, block_size=32, n_head=4, n_layers=4, mask=True):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, n_embd)
        self.position_embedding = nn.Embedding(block_size, n_embd)
        self.dec_blocks = nn.Sequential(*[DecoderBlock(n_embd, block_size, n_head, mask) for  _ in range(n_layers)])
        self.ln = nn.LayerNorm(n_embd)
        self.linear = nn.Linear(n_embd, vocab_size)


    def forward(self, idx, targets = None):
        B, T = idx.shape

        tok_emb = self.token_embedding(idx)
        pos_emb = self.position_embedding(torch.arange(T, device=device))
        x = tok_emb + pos_emb
        x = self.dec_blocks(x)
        x = self.ln(x)
        logits = self.linear(x)
        if targets is not None:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)
        else:
            loss = None
        return logits, loss
    

    def generate(self, idx, num_tokens=100, block_size=32):
        for _ in range(num_tokens):
            idx_cond =  idx[:, -block_size:]
            logits, loss = self(idx_cond)
            logits = logits[:, -1, :]
            probs = F.softmax(logits, dim=-1)
            idx_next =  torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, idx_next), dim=1)
        return idx


# model-initialization

m = GPT(vocab_size, block_size = block_size, n_embd = n_embd)
m.to(device)
optimizer =  torch.optim.AdamW(m.parameters(),  lr=learning_rate)

# model-training

st = time.time()

for step in range(steps):
    x, y = data_loader('train')
    logits, loss = m(x, y)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()
    if step % eval_step == 0:
      train_loss, val_loss = get_loss(m)
      print(f'Step {step}:  Train Loss: {train_loss.item():.4f},  Val Loss: {val_loss.item():.4f}')

et = time.time()

mins = (et - st)   //  60
secs = int((et - st) % 60)

print()
print(f'Time Elasped: {mins} mins {secs} secs')

# output-generation

with torch.no_grad():
    input =  torch.tensor(tokenizer.encode('\n')).to(device).view(1, -1)
    output = m.generate(idx = input, num_tokens=10000, block_size=block_size)
    output = tokenizer.decode(output[0].tolist())

with open('data/output.txt', 'w') as f:
  f.write(output)

