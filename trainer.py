# -*- coding: utf-8 -*-
"""trainer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jzC-jR71ywUScSkGP8BEMz2iHh_ibIaK
"""

#  !pip install tiktoken

import tiktoken
import torch
from Transformer import LanguageModel
import time

tokenizer = tiktoken.get_encoding('cl100k_base')

#-------
with open('input.txt',  'r') as f:
    text = f.read()

# train/val split
enc_text = tokenizer.encode(text)
split = int(0.9 * len(enc_text))
train_data = enc_text[:split]
val_data = enc_text[split:]
#------

# hyperparameters

vocab_size = tokenizer.n_vocab
device = 'cuda' if torch.cuda.is_available() else 'cpu'
n_embd = 32
block_size = 128
batch_size = 32
learning_rate = 1e-4
steps = 20000
eval_step = steps // 10

def data_loader(split='train'):
    data = train_data if split=='train' else val_data
    rand_nums = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([torch.tensor(data[i:i+block_size]) for i in rand_nums]).to(device)
    y = torch.stack([torch.tensor(data[i+1:i+block_size+1]) for i in rand_nums]).to(device)
    return x, y

@torch.no_grad
def get_loss(m):
    train_lossi = []
    val_lossi = []
    m.eval()
    for _ in range(100):
        # train
        x, y = data_loader('train')
        logits, loss = m(x, y)
        train_lossi.append(loss)

        # val
        x, y = data_loader('val')
        logits, loss = m(x, y)
        val_lossi.append(loss)
    train_loss = torch.tensor(train_lossi).mean()
    val_loss = torch.tensor(val_lossi).mean()
    m.train()
    return train_loss, val_loss

m = LanguageModel(vocab_size, block_size = block_size, n_embd = n_embd)
m.to(device)
optimizer =  torch.optim.AdamW(m.parameters(),  lr=learning_rate)

st = time.time()

for step in range(steps):
    x, y = data_loader('train')
    logits, loss = m(x, y)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()
    if step % eval_step == 0:
      train_loss, val_loss = get_loss(m)
      print(f'Step {step}:  Train Loss: {train_loss.item():.4f},  Val Loss: {val_loss.item():.4f}')

et = time.time()

mins = (et - st)   //  60
secs = int((et - st) % 60)

print()
print(f'Time Elasped: {mins} mins {secs} secs')

with torch.no_grad():
    input =  torch.tensor(tokenizer.encode('\n')).to(device).view(1, -1)
    output = m.generate(idx = input, num_tokens=10000, block_size=block_size)
    output = tokenizer.decode(output[0].tolist())

with open('output.txt', 'w') as f:
  f.write(output)

